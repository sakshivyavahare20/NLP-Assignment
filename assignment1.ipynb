{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddabc6c6",
   "metadata": {},
   "source": [
    "Assignment 1\n",
    "Tokenization + Stemming + Lemmatization (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f895e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1: Tokenization, Stemming, Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f900f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('treebank')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2f36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a leading platform for building Python programs to work with human language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d0e37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language.']\n"
     ]
    }
   ],
   "source": [
    "# 1. Whitespace Tokenization\n",
    "whitespace_tokens = text.split()\n",
    "print(\"Whitespace Tokens:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f64ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation Tokens: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. Punctuation-based Tokenization\n",
    "punct_tokens = word_tokenize(text)\n",
    "print(\"Punctuation Tokens:\", punct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e858496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokens: [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Treebank Tokenization\n",
    "treebank_tokens = treebank.tagged_sents()[0]\n",
    "print(\"Treebank Tokens:\", treebank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c1962c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokens: ['Learning', 'NLP', 'is', 'fun', '!', '#AI', '@OpenAI']\n"
     ]
    }
   ],
   "source": [
    "# 4. Tweet Tokenization\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(\"Learning NLP is fun! #AI @OpenAI\")\n",
    "print(\"Tweet Tokens:\", tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39763969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokens: ['I', 'live', 'in', 'New_York', 'and', 'study', 'machine_learning']\n"
     ]
    }
   ],
   "source": [
    "# 5. Multi-Word Expression Tokenization\n",
    "mwe = MWETokenizer([('New', 'York'), ('machine', 'learning')])\n",
    "mwe_tokens = mwe.tokenize(\"I live in New York and study machine learning\".split())\n",
    "print(\"MWE Tokens:\", mwe_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d9aca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58ade97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', '.']\n",
      "Snowball Stemmer: ['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', '.']\n",
      "Lemmatized Words: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter Stemmer:\", [porter.stem(w) for w in punct_tokens])\n",
    "print(\"Snowball Stemmer:\", [snowball.stem(w) for w in punct_tokens])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemmatized Words:\", [lemmatizer.lemmatize(w) for w in punct_tokens])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
